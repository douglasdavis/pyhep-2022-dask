{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955586f6",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" align=\"right\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02064b8d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>\n",
    "Dask Tutorial for PyHEP 2022\n",
    "    </h1>\n",
    "</center>\n",
    "\n",
    "Dask is a pure Python library for parallel and distributed computing designed to scale up workflows in the PyData ecosystem.\n",
    "\n",
    "The two main components of Dask:\n",
    "\n",
    "- The **collections library(ies)** (sometimes called \"Dask core\") listed here in alphabetical order:\n",
    "  - `dask.array`: chunked NumPy\n",
    "  - `dask.bag`: partitioned Python iterables\n",
    "  - `dask.dataframe`: partitioned Pandas\n",
    "  - `dask.delayed`: custom algorithms\n",
    "- The **execution engines** (task schedulers)\n",
    "  - The distributed engine is its own project (`distributed`, sometimes called \"Dask Distributed\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911fe3d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://docs.dask.org/en/stable/_images/dask-overview.svg\" align=\"center\" width=\"70%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353aed9-95b3-4f5f-ba63-3ab6e6edd55c",
   "metadata": {},
   "source": [
    "There are also a number of other projects in the Dask ecosystem that leverage both upstream components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac9a3c-fb91-46a2-b1db-4d1f28932032",
   "metadata": {},
   "source": [
    "First Example (using `dask.delayed`)\n",
    "------------------------------------\n",
    "\n",
    "We'll start with a simple `dask.delayed` example that covers _a lot_ of how Dask works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.delayed import delayed\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "inc = delayed(inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49504827-e768-4482-9dde-7a1232a74e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14288fe8-6993-4533-aa20-3172a52292d3",
   "metadata": {},
   "source": [
    "Notice that this just creates a `Delayed` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab375e37-28f8-43db-9e8e-f4f05bba5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "eight = inc(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60660ca-ab55-4de4-93c3-50fc3581a091",
   "metadata": {},
   "source": [
    "We have to ask Dask to determine the result via `compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fc2c1-4af0-4ca7-9292-2ab8865ce317",
   "metadata": {},
   "outputs": [],
   "source": [
    "eight.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32581a4-266e-4b58-b599-00824cc0ad69",
   "metadata": {},
   "source": [
    "We can start to construct a more complex task graph by chaining function calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5819f9-196d-4b61-9904-64dca6bd4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "@delayed\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac52b4-b898-46b6-9831-63a5efbcf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "five = add(inc(1), inc(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56b62e-89cd-4c2d-b66e-9a0b32974297",
   "metadata": {},
   "outputs": [],
   "source": [
    "five.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942e764-650c-4651-a61d-d150766e95d7",
   "metadata": {},
   "source": [
    "We can inspect the complete task graph to see how dask accomplishing computing the result of the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fa2b9-4b6b-47c8-a323-062f376f504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_task_graph = five.dask.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae648d-c729-41e5-aa6d-aef534fd8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(delayed_task_graph.items()):\n",
    "    if i != 0:\n",
    "        print(\"\\n\")\n",
    "    print(\"The key (label) of a task:   \", k)\n",
    "    print(\"The task itself (Lisp S-exp):\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fe781-1b82-4172-83e4-7bdb75af785f",
   "metadata": {},
   "source": [
    "There is a much better method of inspection! (`visualize()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d050846b-e0a7-47e3-bd52-bdd3998d5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "five.visualize(engine=\"cytoscape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff30f9-f82c-4422-90da-c27acb8976be",
   "metadata": {},
   "source": [
    "Second Example\n",
    "--------------\n",
    "\n",
    "Let's take a look at an example that illustrates something closer to a real workflow: reading and operating on files to produce a histogram. Our example will have two steps:\n",
    "\n",
    "1. Load an uproot TTree by file and tree name\n",
    "2. Calculate something from information in the file\n",
    "3. Histogram the calculation\n",
    "\n",
    "We'll look at the workflow while leveraging Dask, and compare to a workflow without Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089d84d-fa4b-4e6b-8ae9-1b5946bcd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import hist\n",
    "import time\n",
    "from skhep_testdata import data_path\n",
    "paths = [data_path(\"uproot-Zmumu.root\")] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea99418-0bc0-4d82-ad2c-1177efd72655",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def read_tree(file_name, tree_name):\n",
    "    time.sleep(1)  # faking making the file larger\n",
    "    return uproot.open(file_name)[tree_name]\n",
    "\n",
    "@delayed\n",
    "def calculation(tree):\n",
    "    arrs = tree.arrays()\n",
    "    return abs(arrs.E1 - arrs.E2)\n",
    "    \n",
    "@delayed\n",
    "def histo(data, bins, range):\n",
    "    h = hist.Hist(hist.axis.Regular(bins=bins, start=range[0], stop=range[1], name=\"abs(E1-E2)\"))\n",
    "    h.fill(data)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993192bc-eb69-4cb6-9a97-1fcefbedd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "histos = []\n",
    "for p in paths:\n",
    "    tree = read_tree(p, \"events\")\n",
    "    calc = calculation(tree)\n",
    "    h = histo(calc, 20, (0, 200))\n",
    "    histos.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b78483-2e25-4bb3-8786-748fc1d587f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "histos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78732b6c-4fb1-4f6f-8c9b-8a136f863e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(histos).visualize(engine=\"cytoscape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35962701-6ef3-4eb3-84d7-a80b839c694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(histos).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13467d3-7526-4bea-9b39-783a43cdf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sum(histos).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c778f80-3ff1-4271-8841-ee68f303ede7",
   "metadata": {},
   "source": [
    "Now without Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe157a2-746f-4da1-b130-00b1509eba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_read_tree(file_name, tree_name):\n",
    "    time.sleep(1)  # faking making the file larger\n",
    "    return uproot.open(file_name)[tree_name]\n",
    "\n",
    "def s_calculation(tree):\n",
    "    arrs = tree.arrays()\n",
    "    return abs(arrs.E1 - arrs.E2)\n",
    "    \n",
    "def s_histo(data, bins, range):\n",
    "    h = hist.Hist(hist.axis.Regular(bins=bins, start=range[0], stop=range[1], name=\"abs(E1-E2)\"))\n",
    "    h.fill(data)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5799c62-961a-40a2-a3a7-de05e06aad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "s_histos = []\n",
    "for p in paths:\n",
    "    tree = s_read_tree(p, \"events\")\n",
    "    calc = s_calculation(tree)\n",
    "    h = histo(calc, 20, (0, 200))\n",
    "    s_histos.append(h)\n",
    "sum(s_histos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7ff9c-2482-413e-9022-37d8ab08e797",
   "metadata": {},
   "source": [
    "dask.array\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5d651-1301-4ed5-8eb6-913a1e8a936d",
   "metadata": {},
   "source": [
    "While `dask.delayed` is incredibly flexible and can turn almost any Python function into a node in a task graph, the other collection libraries are designed to provide task graph creation as a near drop in replacement to existing PyData libraries. The NumPy API is meant to be recreated with `dask.array`. Arrays in `dask.array` are chunked and lazily evaluated NumPy arrays. The data nodes in a task graph are just NumPy arrays: Dask doesn't create a new array computation kernel library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540970af-9a0b-4abb-bcba-f1c0743f1546",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-array.svg\" width=\"50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00ba32-ea2a-49f9-8070-cf2c5e660513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e4275-57ee-4404-b0eb-7205ca8da22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.ones((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f3cbb-4f16-48ab-a2f7-c6c723b2b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415275df-293a-4e76-b4ea-4a53e2bedbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1[:5].sum() + a1[5:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05715e67-f41d-400e-af86-1aa119f68fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = da.ones((10,), chunks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d094fb5-ab19-4c41-945a-d6e2fce87c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c451060-d43b-434e-b110-cd5c404fd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f3da7-c362-456f-beed-e8f847540f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = a2 + 1\n",
    "dask.compute(a2.sum(), a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a28840-7d84-4283-9a11-344c81e74d5d",
   "metadata": {},
   "source": [
    "Chaining together function calls with `dask.array` is very similar to what we did with `dask.delayed`: it simply builds up the task graph. However, now we get to use the ubiquitous NumPy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15795bb5-0f26-4141-9531-cc4f2305774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.ones((25000, 25000), chunks=(5000, 5000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878d855-fbc6-433d-8272-014f022fc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x.T\n",
    "z = da.mean(y[::2, :5000:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb8c63-36b6-44ec-bc5c-c105d9259eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(engine=\"cytoscape\", color=\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69568f-8ccf-4a52-8311-8d87fba9963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(engine=\"cytoscape\", color=\"order\", optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117880f-4ac9-46e6-a212-7b2486da9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323687f0-80d3-4b00-88ad-8705cc13f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e762fc-f411-4432-9ac7-b77c3e1dadc7",
   "metadata": {},
   "source": [
    "dask.dataframe\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5840fec-f327-48f0-89ab-e65f432c395a",
   "metadata": {},
   "source": [
    "The NumPy/dask.array relationship is mirrored for Pandas with dask.dataframe. DataFrames(Series) in dask.dataframe are partitioned and lazily evalualated Pandas DataFrames(Series). The data nodes in a task graph are pandas objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fbf1c2-08ad-431a-a51c-0775730b6b0b",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-dataframe.svg\" width=\"35%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf86758-c204-4c75-9acb-0f236c1b2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.datasets import timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f29b4-4931-4288-b897-63f3a1500ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4f534-e33f-4d3d-b516-58f486f8a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f4169c-a4ef-4e20-9485-41a845c7869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.groupby(\"name\")[[\"x\", \"y\"]].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e2474-41f0-4b4a-9c67-281d7ab0e65e",
   "metadata": {},
   "source": [
    "distributed\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0587960-7ffd-4604-8601-8f8d8d1dfe96",
   "metadata": {},
   "source": [
    "So far we've been using the default execution engine at each `.compute()` call: we're using all discovered threads. This parallelizes computation on a laptop, for example.\n",
    "\n",
    "The distributed engine uses a client + cluster model, where the cluster is composed of a scheduler and any number of workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a89ac-1ca9-4176-9e53-50efdd90cfc7",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/distributed.png\" width=60%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028f21b-a6b6-4504-925e-1c1348fd9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e6096-4b84-4692-8acc-d6e05cdc6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e8e1a-18bd-4df9-8224-16dd2cb1ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42fc09-69a3-4ee0-a4b2-d97e18e52881",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a4041-ef69-4ea5-ba6b-c80ddb9a0593",
   "metadata": {},
   "source": [
    "The ecosystem of clusters is large!\n",
    "\n",
    "- [dask-jobqueue](https://jobqueue.dask.org/en/latest/index.html) (HTCondor, Slurm, PBS, and more)\n",
    "- [dask-cloudprovider](https://docs.dask.org/en/latest/deploying-cloud.html)\n",
    "- [dask-kubernetes](https://kubernetes.dask.org/en/latest/)\n",
    "- [dask-gateway](https://gateway.dask.org/)\n",
    "- ...\n",
    "\n",
    "Simplified example:\n",
    "\n",
    "```python\n",
    "from distributed import Client\n",
    "from dask_jobqueue import HTCondorCluster\n",
    "cluster = HTCondorCluster(memory=\"2GB\")\n",
    "cluster.scale(10)\n",
    "client = Client(cluster)\n",
    "\n",
    "discovery = client.compute(my_big_task)\n",
    "```\n",
    "\n",
    "**Note for folks using binder**: the dashboard part of the live tutorial will be unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00383571-49e3-4cc7-8e44-e494645d8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92b229-d9d6-4eab-89e1-99bec50ba500",
   "metadata": {},
   "source": [
    "Adding to the ecosystem!\n",
    "========================\n",
    "\n",
    "[dask-awkward](https://github.com/ContinuumIO/dask-awkward) and [dask-histogram](https://github.com/dask-contrib/dask-histogram): Native support for awkward arrays and boost-histogram objects in Dask. To showcase these projects we'll work out a small example\n",
    "\n",
    "**Note**: dask-awkward depends on Awkward Array version 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69723e9b-23c1-4a48-9c6f-f13846baeae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "import dask_histogram as dh\n",
    "import awkward._v2 as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9bcd4e-502a-4682-b935-375402cde576",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dak.from_parquet(\n",
    "    \"s3://ddavistemp/higgs_pq/*.parquet\",\n",
    "    storage_options={\"anon\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfae9c5-5203-4b6b-a04b-c801203657f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c5296-82de-45c2-b0d2-f64333f89868",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_muons = ds.muons[dak.num(ds.muons, axis=1) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e62ce9-aa93-466f-aef3-f62cb2925154",
   "metadata": {},
   "outputs": [],
   "source": [
    "leading_pt = dak.max(two_muons.pt, axis=1).map_partitions(ak.fill_none, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563556b-2565-498b-a73f-97ca30c298eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = leading_pt > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ac955-8a95-4076-b4a6-e40a3654b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_pt = dak.min(two_muons[cut].pt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e82642-d945-4118-878f-fb69fa1ecc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dh.histogram(second_pt, bins=20, range=(0, 70), histogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7aa2cc-8a40-4154-8786-16e1ae44e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.Hist(h.compute()).plot1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273fb28-66d5-4c52-9c30-8c34dec26e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de476040-4867-4705-89de-103dfc0608b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0523bb-3337-4d7d-95da-cdeb18def554",
   "metadata": {},
   "source": [
    "Final Thoughts\n",
    "==============\n",
    "We've just scratched the surface of the Dask universe, but hopefully you've learn something! More resources for Dask in general:\n",
    "\n",
    "- [The Dask documentation](https://docs.dask.org/en/stable/)\n",
    "- [The official Dask Tutorial](https://github.com/dask/dask-tutorial) (multi hour tutorial)\n",
    "\n",
    "The [dask-awkward project](https://github.com/ContinuumIO/dask-awkward) welcomes users willing to live a bit on the edge! Please stay tuned for follow up presentations showcasing more Dask <--> Scikit-HEP work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107569c8-2ea3-46dd-b0b4-c1ce42ea0ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
